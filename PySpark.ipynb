{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc3766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07b275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d57c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 20:42:30 WARN Utils: Your hostname, Alirezas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.202 instead (on interface en0)\n",
      "23/04/01 20:42:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 20:42:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName('PySpark Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada3335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.202:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc3c00712e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1afb41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame (1st approach)\n",
    "#df_pandas = pd.read_csv('test1.csv')\n",
    "df_pyspark = spark.read.csv('test1.csv')\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2132c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    _c0|_c1|      _c2|\n",
      "+-------+---+---------+\n",
      "|   Name|Age|      DoB|\n",
      "|Alireza| 18|Apr25XXXX|\n",
      "|Yasaman| 18|Feb28XXXX|\n",
      "|Artemis|  1|Apr16XXXX|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c1d19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, DoB: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame (2nd approach)\n",
    "df_pyspark = spark.read.option('header', 'true').csv('test1.csv')\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf53544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|      DoB|\n",
      "+-------+---+---------+\n",
      "|Alireza| 18|Apr25XXXX|\n",
      "|Yasaman| 18|Feb28XXXX|\n",
      "|Artemis|  1|Apr16XXXX|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e30f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2358504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- DoB: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793b096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- DoB: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame (3rd and best approach)\n",
    "df_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ac470f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'DoB']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return column headers \n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "895869d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Alireza', Age=18, DoB='Apr25XXXX'),\n",
       " Row(Name='Yasaman', Age=18, DoB='Feb28XXXX'),\n",
       " Row(Name='Artemis', Age=1, DoB='Apr16XXXX')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be0e416",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df_pandas['name']\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_pyspark\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "# df_pandas['name']\n",
    "df_pyspark['Name'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7bdf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|Alireza|\n",
      "|Yasaman|\n",
      "|Artemis|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb71f9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('DoB', 'string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13a812cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+---------+\n",
      "|summary|   Name|               Age|      DoB|\n",
      "+-------+-------+------------------+---------+\n",
      "|  count|      3|                 3|        3|\n",
      "|   mean|   null|12.333333333333334|     null|\n",
      "| stddev|   null| 9.814954576223638|     null|\n",
      "|    min|Alireza|                 1|Apr16XXXX|\n",
      "|    max|Yasaman|                18|Feb28XXXX|\n",
      "+-------+-------+------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f969d8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# add column to the dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_pyspark \u001b[38;5;241m=\u001b[39m \u001b[43mdf_pyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDoB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApr2519XX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeb2119XX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApr1620XX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:3035\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m-> 3035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[0;31mTypeError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "# add column to the dataframe\n",
    "df_pyspark = df_pyspark.withColumn('DoB', ['Apr2519XX', 'Feb2119XX', 'Apr1620XX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92b572bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|Age|      DoB|\n",
      "+---+---------+\n",
      "| 18|Apr25XXXX|\n",
      "| 18|Feb28XXXX|\n",
      "|  1|Apr16XXXX|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop column from the dataframe\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93078923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+\n",
      "|FirstName|Age|      DoB|\n",
      "+---------+---+---------+\n",
      "|  Alireza| 18|Apr25XXXX|\n",
      "|  Yasaman| 18|Feb28XXXX|\n",
      "|  Artemis|  1|Apr16XXXX|\n",
      "+---------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename the columns\n",
    "df_pyspark.withColumnRenamed('Name', 'FirstName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac5d6ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, DoB: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop null values\n",
    "df_pyspark.na.drop() # default is how='any'\n",
    "df_pyspark.na.drop(how='all')\n",
    "df_pyspark.na.drop(thresh=2) # if at least 2 non-null values exist, it willt drop the row\n",
    "df_pyspark.na.drop(subset=['DoB']) # drop if null exists in subset columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88b2d35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, DoB: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill missing values\n",
    "df_pyspark.na.fill('MissingValues') # fill all null values with \"MissingValues\"\n",
    "df_pyspark.na.fill('MissingValue', 'DoB') # fill null values in \"DoB\" column with \"MissingValues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d46bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with \"mean\" (or \"mode\", or \"median\")\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age'],\n",
    "    outputCols = ['{}_imputed'.format(c) for c in ['Age']]\n",
    "    ).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e09d936a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[Name: string, Age: int, DoB: string, Age_imputed: int]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13889b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "654396c8",
   "metadata": {},
   "source": [
    "### Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6a6b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|      DoB|\n",
      "+-------+---+---------+\n",
      "|Alireza| 18|Apr25XXXX|\n",
      "|Yasaman| 18|Feb28XXXX|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"Age > 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1193e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|      DoB|\n",
      "+-------+---+---------+\n",
      "|Alireza| 18|Apr25XXXX|\n",
      "|Yasaman| 18|Feb28XXXX|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Age'] > 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d3b0801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|      DoB|\n",
      "+-------+---+---------+\n",
      "|Yasaman| 18|Feb28XXXX|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Age'] > 2) & (df_pyspark['Name'] == 'Yasaman')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf8414e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|      DoB|\n",
      "+-------+---+---------+\n",
      "|Alireza| 18|Apr25XXXX|\n",
      "|Yasaman| 18|Feb28XXXX|\n",
      "|Artemis|  1|Apr16XXXX|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Age'] > 2) | (df_pyspark['Name'] == 'Artemis')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e8eb81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|      DoB|\n",
      "+-------+---+---------+\n",
      "|Artemis|  1|Apr16XXXX|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Age'] > 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "218a2278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   Name|      DoB|\n",
      "+-------+---------+\n",
      "|Alireza|Apr25XXXX|\n",
      "|Yasaman|Feb28XXXX|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Age'] > 2).select(['Name', 'DoB']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a637dd",
   "metadata": {},
   "source": [
    "### GroupBy and Agrregtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec02b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b51c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.agg({'Salary': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bda31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de0c4475",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ML').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07601e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "spark_df = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a717313",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ea4b7",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f4f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
